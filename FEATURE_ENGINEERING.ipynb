{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE ENGINEERING{ASSIGNMENT}"
      ],
      "metadata": {
        "id": "QhTiKC6zxuy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 1 What is a parameter?"
      ],
      "metadata": {
        "id": "BtJE9NhvyCt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> It is a variable that is used to control the behavior of a model or an algorithm. It is a value that is set before training a model, and it can significantly impact the performance of the model.\n"
      ],
      "metadata": {
        "id": "oqD2LzCHyjCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2 What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "4J5FXIGRyug3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Correlation is a statistical measure that describes the relationship between two variables. It measures how closely the values of one variable are associated with the values of another variable. Correlation can be positive, negative, or neutral (no correlation).\n",
        "\n",
        "-> A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable tends to decrease. In other words, when one variable goes up, the other variable tends to go down.\n",
        "\n",
        "For example, let's say there's a negative correlation between the amount of rain and the number of people visiting a park. As the amount of rain increases, the number of people visiting the park tends to decrease"
      ],
      "metadata": {
        "id": "6LiT-rHYy7SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 3 Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "xu-jfoEB0ZHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables systems to automatically improve their performance on a task without being explicitly programmed. It involves training algorithms on data to learn patterns, relationships, and decision-making rules, allowing the system to make predictions, classify objects, or make recommendations.\n",
        "\n",
        " The main Components of Machine Learning:\n",
        "\n",
        "1. **Data**: The foundation of ML, data is used to train and test models. It can be structured (e.g., tables, databases) or unstructured (e.g., text, images).\n",
        "2.** Algorithms**: ML algorithms are the core of the learning process. They analyze the data, identify patterns, and make predictions or decisions. Common algorithms include decision trees, neural networks, and support vector machines.\n",
        "3. **Model**: A trained ML model is a mathematical representation of the relationships learned from the data. It can be used to make predictions, classify new data, or generate insights.\n",
        "4. **Training**: The process of feeding data to an ML algorithm to learn patterns and relationships.\n",
        "5. **Testing**: Evaluating the performance of a trained model on unseen data to assess its accuracy and generalizability.\n"
      ],
      "metadata": {
        "id": "nYSiGyVN0fIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 4 How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "BfptnMrX1YTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In machine learning, a loss function measures the difference between the model's predictions and the actual true values. The loss value is a numerical representation of this difference.\n",
        "\n",
        "A lower loss value indicates that the model's predictions are closer to the true values, suggesting a better fit. Conversely, a higher loss value indicates a poorer fit.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "1. Training Loss: A decreasing training loss during training indicates that the model is learning from the data.\n",
        "2. Validation Loss: A low validation loss indicates that the model generalizes well to unseen data. A high validation loss may indicate overfitting\n"
      ],
      "metadata": {
        "id": "82I3lc5d1jQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 5 What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "6z5vGAxE2hT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> **Continuous Variables**:\n",
        "\n",
        "Continuous variables are numerical variables that can take on any value within a given range or interval. They can be measured with precision and can have any value, including fractions or decimals. Examples:\n",
        "\n",
        "1. Height (e.g., 175.5 cm)\n",
        "2. Weight (e.g., 70.2 kg)\n",
        "3. Temperature (e.g., 23.4°C)\n",
        "\n",
        "**Categorical Variables**:\n",
        "\n",
        "Categorical variables are variables that take on distinct, non-numerical values or categories. They represent groupings or classifications. Examples:\n",
        "\n",
        "1. Color (e.g., red, blue, green)\n",
        "2. Sex (e.g., male, female)\n",
        "3. Product category (e.g., electronics, clothing, books)\n"
      ],
      "metadata": {
        "id": "dsuSQ5yN2phq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 6 How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "8n222l4e2_Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Categorical variables need to be encoded or transformed into numerical representations that machine learning algorithms can understand. Here are some common techniques:\n",
        "\n",
        "1. Label Encoding: Assigns a unique integer value to each category.\n",
        "2. One-Hot Encoding (OHE): Creates a new binary column for each category, with a 1 indicating the presence of that category and a 0 otherwise.\n",
        "3. Ordinal Encoding: Similar to label encoding, but preserves the order or hierarchy between categories.\n",
        "4. Binary Encoding: Represents categories as binary numbers, reducing dimensionality compared to OHE.\n",
        "5. Hashing: Uses a hash function to map categories to numerical values.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I8NeSOL83Iqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 7 What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "wZrFKIjB32_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In machine learning, a dataset is typically split into two parts:\n",
        "\n",
        "1. Training Set: A portion of the dataset used to train a model. The model learns patterns and relationships from this data.\n",
        "2. Testing Set: A separate portion of the dataset used to evaluate the trained model's performance. The model makes predictions on this data, and its performance is measured.\n"
      ],
      "metadata": {
        "id": "qf6UWY254GR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 8 What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "v1rGxF7Q4QyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library. It provides various tools for preprocessing and transforming data, making it suitable for machine learning models.\n"
      ],
      "metadata": {
        "id": "OWHJeLbF4Y75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 9 What is a Test set?"
      ],
      "metadata": {
        "id": "wYd2Cz0W4ge7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A test set, also known as a holdout set or evaluation set, is a portion of a dataset that is used to evaluate the performance of a machine learning model. It is a set of data that the model has not seen during training and is used to assess how well the model generalizes to new, unseen data.\n"
      ],
      "metadata": {
        "id": "0lO6ElHf4l9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 10 How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "_HU0ehaB2uNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In Python, you can use the train_test_split function from scikit-learn to split your data into training and testing sets:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kxtnaoC123D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Initialize dummy variables for X and y\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Assuming X is your feature data and y is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XaECIMMw3rB_",
        "outputId": "0f0c242b-5b1c-433a-aac5-0f1d384a2b84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "X_test: [[3 4]]\n",
            "y_train: [0 0 0 1]\n",
            "y_test: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approaching a Machine Learning Problem:\n",
        "\n",
        "1. Define the Problem: Clearly define the problem you're trying to solve and the goals of your project.\n",
        "2. Collect and Preprocess Data: Collect relevant data and preprocess it to ensure it's clean, complete, and in a suitable format for modeling.\n",
        "3. Split Data: Split your data into training and testing sets.\n",
        "4. Choose a Model: Select a suitable machine learning algorithm based on the problem type (classification, regression, etc.) and data characteristics.\n",
        "5. Train the Model: Train the model using the training data.\n",
        "6. Evaluate the Model: Evaluate the model's performance using the testing data and relevant metrics.\n",
        "7. Tune Hyperparameters: Tune the model's hyperparameters to improve its performance.\n",
        "8. Deploy the Model: Deploy the trained model in a suitable environment.\n"
      ],
      "metadata": {
        "id": "LBUOZW6d4C8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 11 Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "qonj_R7p4H1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> EDA is a crucial step in the data science process that involves analyzing and visualizing data to understand its underlying structure, patterns, and relationships.\n",
        "\n",
        "Performing EDA before fitting a model to the data is essential because it:\n",
        "\n",
        "1. Helps Understand Data Distribution: EDA reveals the distribution of variables, including skewness, outliers, and correlations.\n",
        "2. Identifies Data Quality Issues: EDA detects missing values, duplicates, and inconsistencies in the data.\n",
        "3. Informs Feature Engineering: EDA provides insights into which features are relevant and how they can be transformed or engineered.\n",
        "4. Guides Model Selection: EDA helps choose the most suitable model based on the data's characteristics and relationships.\n",
        "5. Improves Model Performance: By understanding the data, you can develop more effective models that capture the underlying patterns and relationships.\n"
      ],
      "metadata": {
        "id": "AaejOaR74MZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 12 What is correlation?"
      ],
      "metadata": {
        "id": "jOkOamYH4YGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Correlation is a statistical measure that describes the relationship between two variables. It measures how closely the values of one variable are associated with the values of another variable.\n"
      ],
      "metadata": {
        "id": "_5W8w6rl4gBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 13 What does negative correlation mean?"
      ],
      "metadata": {
        "id": "3I--fOfS4oqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable tends to decrease. In other words, when one variable goes up, the other variable tends to go down.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's say there's a negative correlation between the amount of rain and the number of people visiting a park. As the amount of rain increases, the number of people visiting the park tends to decrease.\n"
      ],
      "metadata": {
        "id": "9cFZjwD34tVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 14 How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "Sn7aHGtB48NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> To find the correlation between variables in Python, you can use libraries like NumPy, pandas, and SciPy. Here are some ways to do it:\n",
        "\n",
        "Methods:\n",
        "\n",
        "- Pearson's Correlation Coefficient: Measures linear correlation between two continuous variables. You can calculate it using pearsonr() from SciPy.\n",
        "- Spearman's Correlation Coefficient: Measures rank correlation between two variables. You can calculate it using spearmanr() from SciPy.\n",
        "- Correlation Matrix: A matrix of correlations between all pairs of variables in a dataset. You can create it using corr() from pandas.\n"
      ],
      "metadata": {
        "id": "esJ4wIN35AZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "# Calculate Pearson's correlation\n",
        "corr, _ = pearsonr(x, y)\n",
        "print(\"Pearson's correlation:\", corr)\n",
        "\n",
        "# Calculate Spearman's correlation\n",
        "from scipy.stats import spearmanr\n",
        "corr, _ = spearmanr(x, y)\n",
        "print(\"Spearman's correlation:\", corr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbA875u_5Met",
        "outputId": "349fde11-9953-4971-bdbd-e0e3b7d1e633"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson's correlation: 1.0\n",
            "Spearman's correlation: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 15 What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "nZRLNYls5TvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Causation, also known as cause-and-effect, refers to a relationship between two variables where one variable (the cause) directly affects the other variable (the effect).\n",
        "\n",
        "**Correlation vs. Causation**:\n",
        "\n",
        "Correlation and causation are often confused, but they're distinct concepts:\n",
        "\n",
        "1. Correlation: A statistical relationship between two variables, where changes in one variable are associated with changes in the other.\n",
        "2. Causation: A direct cause-and-effect relationship between two variables, where one variable influences the other.\n",
        "\n",
        "Example:\n",
        "\n",
        "Let's consider the relationship between ice cream sales and number of people wearing shorts.\n",
        "\n",
        "- Correlation: There's a strong positive correlation between ice cream sales and number of people wearing shorts. When ice cream sales increase, more people wear shorts.\n",
        "- Causation: However, there's no direct causation between ice cream sales and people wearing shorts. The underlying cause is likely the temperature: when it's warm outside, people buy more ice cream and wear shorts.\n"
      ],
      "metadata": {
        "id": "6bpxO-AW5ey4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 16 What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "sThFqrXi6G-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> An optimizer is a crucial component in machine learning and deep learning that adjusts the model's parameters to minimize the loss function, thereby improving the model's performance.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "1. Gradient Descent (GD): Updates parameters based on the gradient of the loss function.\n",
        "2. Stochastic Gradient Descent (SGD): Updates parameters based on the gradient of the loss function for a single sample.\n",
        "3. Mini-Batch Gradient Descent: Updates parameters based on the gradient of the loss function for a small batch of samples.\n",
        "4. Momentum: Adds a momentum term to the update rule to help escape local minima.\n",
        "5. Nesterov Accelerated Gradient: Modifies the momentum update rule to improve convergence.\n",
        "6. Adagrad: Adapts the learning rate for each parameter based on past gradients.\n",
        "7. Adadelta: Modifies Adagrad to reduce the learning rate decay.\n",
        "8. RMSprop: Adapts the learning rate based on the magnitude of recent gradients.\n",
        "9. Adam: Combines Adagrad and RMSprop to adapt the learning rate for each parameter.\n",
        "\n",
        "Examples:\n",
        "\n",
        "1. GD: Used in linear regression to minimize the mean squared error.\n",
        "2. SGD: Used in online learning scenarios where data is streaming in.\n",
        "3. Adam: Used in deep learning models, such as neural networks, to adapt the learning rate and improve convergence"
      ],
      "metadata": {
        "id": "ZhmAYsZm6hmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 17 What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "rCItPP2E68Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> sklearn.linear_model is a module in scikit-learn, a popular Python machine learning library, that provides various linear models for regression, classification, and other tasks.\n",
        "\n",
        "Some Key Models:\n",
        "\n",
        "1. LinearRegression: Linear regression model for predicting continuous outcomes.\n",
        "2. LogisticRegression: Logistic regression model for binary classification.\n",
        "3. Ridge: Ridge regression model with L2 regularization.\n",
        "4. Lasso: Lasso regression model with L1 regularization.\n",
        "5. ElasticNet: Elastic Net regression model with both L1 and L2 regularization."
      ],
      "metadata": {
        "id": "tNoNWzdb7C2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 18 What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "TZzKh2H17gU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> model.fit() is a method in scikit-learn that trains a machine learning model on a given dataset. It adjusts the model's parameters to minimize the loss function and optimize its performance.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "The fit() method typically requires two main arguments:\n",
        "\n",
        "1. X: The feature data (predictors) used to train the model.\n",
        "2. y: The target data (response variable) that the model is trying to predict.\n"
      ],
      "metadata": {
        "id": "lQLeBZ5O8Iir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 19 What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "E2NDmcNg8Nyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> model.predict() is a method in scikit-learn that uses a trained machine learning model to make predictions on new, unseen data.\n",
        "\n",
        "Arguments:\n",
        "\n",
        "The predict() method typically requires one main argument:\n",
        "\n",
        "1. X: The feature data (predictors) for which the model will make predictions.\n"
      ],
      "metadata": {
        "id": "DeCmk84e8VMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 20 What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "0R0CcGkv8rZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Continuous Variables:\n",
        "\n",
        "Continuous variables are numerical variables that can take on any value within a given range or interval. They can be measured with precision and can have any value, including fractions or decimals. Examples:\n",
        "\n",
        "1. Height: 175.5 cm\n",
        "2. Weight: 70.2 kg\n",
        "3. Temperature: 23.4°C\n",
        "\n",
        "Categorical Variables:\n",
        "\n",
        "Categorical variables are variables that take on distinct, non-numerical values or categories. They represent groupings or classifications. Examples:\n",
        "\n",
        "1. Color: red, blue, green\n",
        "2. Sex: male, female\n",
        "3. Product Category: electronics, clothing, books\n"
      ],
      "metadata": {
        "id": "fqw64edm9Ars"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 21 What is feature scaling? How does it help in Machine Learning?\n"
      ],
      "metadata": {
        "id": "6SPotbg19Vjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Feature scaling, also known as data normalization or standardization, is a technique used to rescale the range of independent variables or features in a dataset. It ensures that all features are on a similar scale, which helps in improving the performance and stability of machine learning models.\n",
        "\n",
        "Why Feature Scaling is Important:\n",
        "\n",
        "1. Prevents Feature Dominance: Feature scaling prevents features with large ranges from dominating the model, ensuring that all features contribute equally to the prediction.\n",
        "2. Improves Model Convergence: Feature scaling can improve the convergence speed and stability of some machine learning algorithms, such as gradient-based methods.\n",
        "3. Enhances Model Interpretability: Feature scaling can make it easier to interpret the results of some machine learning models, such as linear regression.\n"
      ],
      "metadata": {
        "id": "NgbCuWT-9e-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 22 How do we perform scalling in python ?"
      ],
      "metadata": {
        "id": "eQbFl8bp-bNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> You can perform scaling in Python using the StandardScaler or MinMaxScaler from the sklearn.preprocessing module.\n",
        "\n",
        "StandardScaler:\n",
        "\n",
        "StandardScaler standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eOg7fF20-4-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoSATdC5_d7k",
        "outputId": "919756ae-6fea-4503-da5f-826b0672ed04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MinMaxScaler:\n",
        "\n",
        "MinMaxScaler scales features to a specified range, usually between 0 and 1.\n"
      ],
      "metadata": {
        "id": "7P8YmIQf_kCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZtLX-wa_3Pp",
        "outputId": "0519787b-fcad-4e38-94fd-403192e5a59a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 23 What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "glNuoNc-_6AK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library, that provides various techniques for preprocessing and transforming data.\n",
        "\n",
        "Some Key Features:\n",
        "\n",
        "1. Scaling: Scaling features to a common range, such as standardization or normalization.\n",
        "2. Encoding: Encoding categorical variables into numerical variables.\n",
        "3. Normalization: Normalizing data to have a specific distribution or range.\n",
        "4. Imputation: Imputing missing values in datasets.\n"
      ],
      "metadata": {
        "id": "0AheHXlBAL-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 24 How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "JCzlkNwaAm-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> You can split data into training and testing sets using the train_test_split function from the sklearn.model_selection module.\n"
      ],
      "metadata": {
        "id": "dCfm3WcXAzP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5aziXldwBPzY",
        "outputId": "1ee949ec-2ca3-43ff-ebfe-c18a85a3c4dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[1 2]\n",
            " [5 6]]\n",
            "X_test: [[3 4]\n",
            " [7 8]]\n",
            "y_train: [0 1]\n",
            "y_test: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 25 Explain data encoding?"
      ],
      "metadata": {
        "id": "9WgyqVyYBTX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Data encoding is the process of converting categorical or textual data into numerical data that can be processed by machine learning algorithms.\n",
        "\n",
        "Why Encode Data:\n",
        "\n",
        "1. Machine Learning Algorithms: Most machine learning algorithms require numerical input data.\n",
        "2. Pattern Recognition: Encoding data helps algorithms recognize patterns and relationships.\n"
      ],
      "metadata": {
        "id": "giK83-fuBYpj"
      }
    }
  ]
}